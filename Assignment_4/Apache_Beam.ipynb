{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Connecting Drive to Colab**"
      ],
      "metadata": {
        "id": "37FBfzDHLgYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNXacxBvna5e",
        "outputId": "1e4f7771-58b4-433c-c81d-97e1165f3c2a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Apache Beam for playing around with Bank Nifty Stock Data**"
      ],
      "metadata": {
        "id": "XG_otUv3Lwt6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Creating pipeline to:**\n",
        "Calculate the monthly average closing price.\n",
        "\n",
        "Calculate the daily volatility."
      ],
      "metadata": {
        "id": "IEcfv6Sm6UUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "from datetime import datetime\n",
        "\n",
        "def extract_month_year(date_str):\n",
        "    \"\"\"Extract month and year from the date string, accounting for double quotes.\"\"\"\n",
        "    cleaned_date_str = date_str.replace('\"', '').strip()\n",
        "    date_obj = datetime.strptime(cleaned_date_str, '%m/%d/%Y')\n",
        "    return f\"{date_obj.month}-{date_obj.year}\"\n",
        "\n",
        "def calculate_volatility(row):\n",
        "    \"\"\"Calculate volatility for the given row, accounting for double quotes.\"\"\"\n",
        "    high = float(row['High'].replace(',', '').replace('\"', ''))\n",
        "    low = float(row['Low'].replace(',', '').replace('\"', ''))\n",
        "    return high - low\n",
        "\n",
        "def run_pipeline():\n",
        "    # Define the columns inside the function to avoid referencing external variables\n",
        "    columns = [\"Date\", \"Price\", \"Open\", \"High\", \"Low\", \"Vol.\", \"Change %\"]\n",
        "\n",
        "    with beam.Pipeline() as p:\n",
        "        rows = (\n",
        "            p | \"ReadFromCSV\" >> beam.io.ReadFromText('/content/drive/MyDrive/Data_Mining_CMPE_255/Nifty_Bank_Data.csv', skip_header_lines=1)\n",
        "              | \"ParseCSV\" >> beam.Map(lambda line: dict(zip(columns, line.split(','))))\n",
        "        )\n",
        "\n",
        "        # Calculate monthly average closing price\n",
        "        monthly_avg = (\n",
        "            rows | \"ExtractMonthYear\" >> beam.Map(lambda row: (extract_month_year(row['Date']), float(row['Price'].replace(',', '').replace('\"', '').strip())))\n",
        "                 | \"GroupByMonthYear\" >> beam.GroupByKey()\n",
        "                 | \"ComputeAveragePrice\" >> beam.Map(lambda kv: (kv[0], sum(kv[1]) / len(kv[1])))\n",
        "                 | \"WriteMonthlyAverage\" >> beam.io.WriteToText('/content/drive/MyDrive/Data_Mining_CMPE_255/apache_monthly_avg.txt')\n",
        "        )\n",
        "\n",
        "        # Calculate daily volatility\n",
        "        volatility = (\n",
        "            rows | \"CalculateVolatility\" >> beam.Map(lambda row: (row['Date'], calculate_volatility(row)))\n",
        "                 | \"WriteVolatility\" >> beam.io.WriteToText('/content/drive/MyDrive/Data_Mining_CMPE_255/apache_volatility.txt')\n",
        "        )\n",
        "\n",
        "run_pipeline()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2Xt2hk9jm3o",
        "outputId": "ba5da6b1-fa9c-4244-bb02-e5fffedf4a20"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n",
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The functions have been succesfully executed and\n",
        "\n",
        "results for monthly average closing price is saved in apache_monthly_avg.txt\n",
        "\n",
        "results for monthly  daily volatility is saved in apache_volatility.txt\n"
      ],
      "metadata": {
        "id": "8UqxhBLZ6qN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Composite Transform:**\n",
        "\n",
        "the sequence of reading from the CSV, parsing the rows, extracting the prices, and computing the monthly averages can be viewed as a composite transform.\n",
        "\n",
        "The results of the consolidated Transform are saved in"
      ],
      "metadata": {
        "id": "I6Fp1RxR-WUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the columns to avoid referencing external variables\n",
        "columns = [\"Date\", \"Price\", \"Open\", \"High\", \"Low\", \"Vol.\", \"Change %\"]\n",
        "\n",
        "class MonthlyAverageComposite(beam.PTransform):\n",
        "    def expand(self, pcoll):\n",
        "        return (\n",
        "            pcoll\n",
        "            | \"ExtractMonthYear\" >> beam.Map(lambda row: (extract_month_year(row['Date']), float(row['Price'].replace(',', '').replace('\"', '').strip())))\n",
        "            | \"GroupByMonthYear\" >> beam.GroupByKey()\n",
        "            | \"ComputeAveragePrice\" >> beam.Map(lambda kv: (kv[0], sum(kv[1]) / len(kv[1])))\n",
        "        )\n",
        "\n",
        "class DailyVolatilityComposite(beam.PTransform):\n",
        "    def expand(self, pcoll):\n",
        "        return (\n",
        "            pcoll\n",
        "            | \"CalculateVolatility\" >> beam.Map(lambda row: (row['Date'], calculate_volatility(row)))\n",
        "        )\n",
        "\n",
        "with beam.Pipeline() as p:\n",
        "    rows = (\n",
        "        p | \"ReadFromCSV\" >> beam.io.ReadFromText('/content/drive/MyDrive/Data_Mining_CMPE_255/Nifty_Bank_Data.csv', skip_header_lines=1)\n",
        "          | \"ParseCSV\" >> beam.Map(lambda line: dict(zip(columns, line.split(','))))\n",
        "    )\n",
        "\n",
        "    # Using the MonthlyAverageComposite transform\n",
        "    monthly_avg = (\n",
        "        rows | \"MonthlyAverage\" >> MonthlyAverageComposite()\n",
        "             | \"WriteMonthlyAverage\" >> beam.io.WriteToText('/content/drive/MyDrive/Data_Mining_CMPE_255/composite_apache_monthly_avg.txt')\n",
        "    )\n",
        "\n",
        "    # Using the DailyVolatilityComposite transform\n",
        "    volatility = (\n",
        "        rows | \"DailyVolatility\" >> DailyVolatilityComposite()\n",
        "             | \"WriteVolatility\" >> beam.io.WriteToText('/content/drive/MyDrive/Data_Mining_CMPE_255/composite_apache_volatility.txt')\n",
        "    )"
      ],
      "metadata": {
        "id": "Xqazq6CdqWVi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Triggers:**\n",
        "In our pipeline, we used AfterCount(10), which means the trigger fires after 10 elements have been processed in a window. This is just a demonstration; in a real streaming scenario, different triggers might be more appropriate.\n",
        "\n",
        "Note: This trigger was flagged as potentially unsafe due to the possibility of data loss, so we added the --allow_unsafe_triggers flag.\n",
        "\n",
        "### **Pardo:**\n",
        " We used ParDo in conjunction with the ProcessWindow class to compute the average price for each window of data. The ProcessWindow class, which is a subclass of DoFn, defines the logic to process each window and compute the average price.\n",
        "\n",
        "### **Windowing**\n",
        "we implemented custom calendar-based windowing logic to group data by month. Instead of using predefined fixed or sliding windows, we used the start of each month as a key for grouping. Each \"window\" in this case corresponds to a month, and the GroupByKey transform effectively creates these monthly windows."
      ],
      "metadata": {
        "id": "xGkVV24ZCDUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Apache Beam in Colab\n",
        "!pip install apache-beam\n",
        "\n",
        "# Import necessary libraries\n",
        "import apache_beam as beam\n",
        "from apache_beam.transforms.core import ParDo, DoFn\n",
        "from datetime import datetime\n",
        "\n",
        "# Define the helper functions and classes\n",
        "def extract_month_year(date_str):\n",
        "    cleaned_date_str = date_str.replace('\"', '').strip()\n",
        "    date_obj = datetime.strptime(cleaned_date_str, '%m/%d/%Y')\n",
        "    return f\"{date_obj.month}-{date_obj.year}\"\n",
        "\n",
        "def month_start_key_str(date_str):\n",
        "    \"\"\"Extract the start of the month as a key for grouping and return as string.\"\"\"\n",
        "    cleaned_date_str = date_str.replace('\"', '').strip()\n",
        "    date_obj = datetime.strptime(cleaned_date_str, '%m/%d/%Y')\n",
        "    return date_obj.replace(day=1).strftime('%Y-%m-%d')\n",
        "\n",
        "class ProcessWindow(DoFn):\n",
        "    def process(self, element, window=DoFn.WindowParam):\n",
        "        key, values = element\n",
        "        avg_price = sum(values) / len(values)\n",
        "        return [(key, avg_price)]\n",
        "\n",
        "# Define the pipeline with custom calendar-based windowing\n",
        "columns = [\"Date\", \"Price\", \"Open\", \"High\", \"Low\", \"Vol.\", \"Change %\"]\n",
        "\n",
        "# Add options to the pipeline to allow unsafe triggers (in case you use any)\n",
        "options = beam.options.pipeline_options.PipelineOptions(flags=['--allow_unsafe_triggers'])\n",
        "\n",
        "with beam.Pipeline(options=options) as p:\n",
        "    rows = (\n",
        "        p | \"ReadFromCSV\" >> beam.io.ReadFromText('/content/drive/MyDrive/Data_Mining_CMPE_255/Nifty_Bank_Data.csv', skip_header_lines=1)\n",
        "          | \"ParseCSV\" >> beam.Map(lambda line: dict(zip(columns, line.split(','))))\n",
        "          | \"ExtractPrice\" >> beam.Map(lambda row: (month_start_key_str(row['Date']), float(row['Price'].replace(',', '').replace('\"', '').strip())))\n",
        "    )\n",
        "\n",
        "    # Using GroupByKey to simulate monthly windows\n",
        "    monthly_avg = (\n",
        "        rows\n",
        "        | \"GroupByKey\" >> beam.GroupByKey()\n",
        "        | \"ProcessWindow\" >> ParDo(ProcessWindow())\n",
        "        | \"WriteResults\" >> beam.io.WriteToText('/content/drive/MyDrive/Data_Mining_CMPE_255/apache_custom_windowed_results.txt')\n",
        "    )\n"
      ],
      "metadata": {
        "id": "e7BmUI66tw7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used Apache Beam to look at Nifty Bank's data and find out the average price for each month. We faced some problems like wrong data formats, but we fixed them. Now, we have a setup that can handle a lot of data and can be used again for other projects. It's a good start for more detailed data studies in the future."
      ],
      "metadata": {
        "id": "IGMr-S0fE5fw"
      }
    }
  ]
}